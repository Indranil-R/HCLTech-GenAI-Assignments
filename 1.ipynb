{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Indranil-R/Silver-Badge-Assignments/blob/main/1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "fedf0bdb052283fb"
      },
      "cell_type": "markdown",
      "source": [
        "# 1. Implement a question answering system with RAG, word embedding, vector database, langchain, llm and any other tools"
      ],
      "id": "fedf0bdb052283fb"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uTvdx4vwzuI",
        "outputId": "fb5ba8b3-9b2d-4172-85ff-bfa683ce3484"
      },
      "id": "_uTvdx4vwzuI",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "initial_id",
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        },
        "id": "initial_id",
        "outputId": "c26f721b-2eaa-4380-a724-e84f9e898953"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_google_genai'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a94ba2420060>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgetenv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_google_genai\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGoogleGenerativeAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenai\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_google_genai'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from os import getenv\n",
        "from langchain_google_genai import GoogleGenerativeAI\n",
        "from google import genai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "19s3rdhYxvRF"
      },
      "id": "19s3rdhYxvRF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = GoogleGenerativeAI(model=\"models/gemini-2.0-flash\",google_api_key=userdata.get('GOOGLE_API_KEY'))\n",
        "print(\n",
        "    llm.invoke(\n",
        "        \"What are some of the pros and cons of Python as a programming language?\"\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "U8rMb2I1wgpY"
      },
      "id": "U8rMb2I1wgpY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate"
      ],
      "metadata": {
        "id": "i6Sflr94xgpU"
      },
      "id": "i6Sflr94xgpU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: What is the answer?.\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "\n",
        "chain = prompt | llm\n",
        "\n",
        "question = \"How much is 2+2?\"\n",
        "\n",
        "# print(chain.invoke({\"question\": question}))"
      ],
      "metadata": {
        "id": "a--YAv6iyKKl"
      },
      "id": "a--YAv6iyKKl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(chain.invoke({\"question\": \"now tell me 2+2\"}))"
      ],
      "metadata": {
        "id": "XLFp-w4OyNAE"
      },
      "id": "XLFp-w4OyNAE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
        "  os.environ[\"GOOGLE_API_KEY\"] = userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "model = init_chat_model(\"gemini-2.0-flash\", model_provider=\"google_genai\")\n",
        "\n",
        "model.invoke(\"Please do not tel me about yourself!\")"
      ],
      "metadata": {
        "id": "oSA7VexqyUP_"
      },
      "id": "oSA7VexqyUP_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wEE_3yu5zoDS"
      },
      "id": "wEE_3yu5zoDS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Embedding from Google API and storing into vector DB and then using it to retreive data"
      ],
      "metadata": {
        "id": "raUjEUdTz_tg"
      },
      "id": "raUjEUdTz_tg"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "import bs4"
      ],
      "metadata": {
        "collapsed": true,
        "id": "d70YTvdy0HgL"
      },
      "id": "d70YTvdy0HgL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the data from Webpage"
      ],
      "metadata": {
        "id": "Ma14W1zY3t94"
      },
      "id": "Ma14W1zY3t94"
    },
    {
      "cell_type": "code",
      "source": [
        "# I am fetching the data from SAP page due to API limitations, keeping the data small\n",
        "loader = WebBaseLoader(\n",
        "    web_paths=(\"https://www.sap.com/india/about/company/faq.html\",),\n",
        "    bs_kwargs=dict(\n",
        "        parse_only=bs4.SoupStrainer(\n",
        "            class_=(\"Grid__col-12--cMZPy Grid__col-sm-12--JrL2M Grid__col-md-12--tG5nF Grid__col-lg-12--wWsJK Grid__col-xl-12--v6LlR \")\n",
        "        )\n",
        "    ),\n",
        ")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "27uHmFk-1qH3"
      },
      "id": "27uHmFk-1qH3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "id": "FgFwCaEF2DJT"
      },
      "id": "FgFwCaEF2DJT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_chroma"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mgxz_1BO2Iog"
      },
      "id": "mgxz_1BO2Iog",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.vectorstores.chroma import Chroma"
      ],
      "metadata": {
        "id": "zYBNMR7A2wa5"
      },
      "id": "zYBNMR7A2wa5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the embedding"
      ],
      "metadata": {
        "id": "tAGnbW6T3z-l"
      },
      "id": "tAGnbW6T3z-l"
    },
    {
      "cell_type": "code",
      "source": [
        "# Google EMbedding model is\n",
        "# models/text-embedding-004\n",
        "\n",
        "from google import genai\n",
        "\n",
        "client = genai.Client(api_key=userdata.get(\"GOOGLE_API_KEY\"))\n",
        "\n",
        "result = client.models.embed_content(\n",
        "        model=\"text-embedding-004\",\n",
        "        contents=\"docs\")\n"
      ],
      "metadata": {
        "id": "VVn-9MQE2wuy"
      },
      "id": "VVn-9MQE2wuy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding the embedding to Chroma VectorDB"
      ],
      "metadata": {
        "id": "CIMzb7_M5Gje"
      },
      "id": "CIMzb7_M5Gje"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3YbKLO-O6vJF"
      },
      "id": "3YbKLO-O6vJF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# check python ver\n",
        "import sys\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "id": "XT5lhtDM4tVo"
      },
      "id": "XT5lhtDM4tVo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chromadb"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t98ljWy_6B2g"
      },
      "id": "t98ljWy_6B2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vector_db = Chroma.from_documents(documents=docs, embedding=result)"
      ],
      "metadata": {
        "id": "wYlL9h8N7O9M"
      },
      "id": "wYlL9h8N7O9M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Switching python ver to 3.9 as chroma is not supported on 3.11\n",
        "# !sudo apt-get install python3.9"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I1tqcoae7ld_"
      },
      "id": "I1tqcoae7ld_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 1"
      ],
      "metadata": {
        "id": "QOC9nb8272ZJ"
      },
      "id": "QOC9nb8272ZJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retreive the data"
      ],
      "metadata": {
        "id": "t50rdEWJ5X16"
      },
      "id": "t50rdEWJ5X16"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain.retrievers.multi_query import MultiQueryRetriever"
      ],
      "metadata": {
        "id": "xfgzDLx45aHn"
      },
      "id": "xfgzDLx45aHn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "QUERY_PROMPT = PromptTemplate(\n",
        "    input_variables=[\"question\"],\n",
        "    template=\"\"\"You are an AI language model assistant. Your task is to generate five\n",
        "    different versions of the given user question to retrieve relevant documents from a vector\n",
        "    database. By generating multiple perspectives on the user question, your goal is to help\n",
        "    the user overcome some of the limitations of the distance-based similarity search.\n",
        "    Provide these alternative questions separated by newlines.\n",
        "    Original question: {question}\"\"\",\n",
        ")\n",
        "\n",
        "retriever = MultiQueryRetriever(\n",
        "    vector_db.as_retriever(search_type=\"mmr\"),\n",
        "    llm=model,\n",
        "    prompt=QUERY_PROMPT,\n",
        "\n",
        ")\n"
      ],
      "metadata": {
        "id": "wcpH9WOh5aby"
      },
      "id": "wcpH9WOh5aby",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "anXegayB5dMA"
      },
      "id": "anXegayB5dMA",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}